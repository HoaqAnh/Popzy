{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d7f9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\xamth\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\xamth\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.23.0)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.9.1-cp310-cp310-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\xamth\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\xamth\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\xamth\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\xamth\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\xamth\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\xamth\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\xamth\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\xamth\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading torchaudio-2.9.0-cp310-cp310-win_amd64.whl.metadata (6.9 kB)\n",
      "  Downloading torchaudio-2.8.0-cp310-cp310-win_amd64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\xamth\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\xamth\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torchaudio-2.8.0-cp310-cp310-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 1.3/2.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.4/2.5 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 4.6 MB/s eta 0:00:00\n",
      "Installing collected packages: torchaudio\n",
      "Successfully installed torchaudio-2.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\xamth\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (11.3.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: tqdm in c:\\users\\xamth\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.67.1)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\xamth\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in c:\\users\\xamth\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opencv-python) (2.1.3)\n",
      "Downloading opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl (39.0 MB)\n",
      "   ---------------------------------------- 0.0/39.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/39.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.3/39.0 MB 4.2 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 2.1/39.0 MB 4.3 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 3.4/39.0 MB 4.6 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 4.5/39.0 MB 4.9 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 5.8/39.0 MB 5.0 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 6.8/39.0 MB 5.0 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 7.6/39.0 MB 4.9 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 8.7/39.0 MB 4.9 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 9.7/39.0 MB 4.9 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 10.5/39.0 MB 4.8 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 11.5/39.0 MB 4.8 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 12.6/39.0 MB 4.8 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 13.6/39.0 MB 4.9 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 14.9/39.0 MB 4.9 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 15.7/39.0 MB 4.9 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 16.8/39.0 MB 4.9 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 17.8/39.0 MB 4.9 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 18.6/39.0 MB 4.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 19.7/39.0 MB 4.8 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 20.7/39.0 MB 4.9 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 21.5/39.0 MB 4.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 22.8/39.0 MB 4.9 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 23.6/39.0 MB 4.9 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 24.6/39.0 MB 4.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 25.4/39.0 MB 4.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 26.2/39.0 MB 4.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 27.3/39.0 MB 4.8 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 27.8/39.0 MB 4.7 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 28.8/39.0 MB 4.7 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 29.6/39.0 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 30.4/39.0 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 31.2/39.0 MB 4.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 32.0/39.0 MB 4.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 32.0/39.0 MB 4.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 33.6/39.0 MB 4.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 34.3/39.0 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 35.4/39.0 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.2/39.0 MB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.2/39.0 MB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.5/39.0 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.8/39.0 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.0/39.0 MB 4.5 MB/s eta 0:00:00\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.12.0.88\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install pillow tqdm opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b265835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "794f7915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Moved Bathroom → bathroom\n",
      "✔️ Moved Bedroom → bedroom\n",
      "✔️ Moved Kitchen → kitchen\n",
      "✔️ Moved Livingroom → living_room\n",
      "✔️ Moved Dinning → other_real_estate\n",
      "✔️ Moved NaturalImages → spam\n",
      "✔️ Moved SpamImages → spam\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "mapping = {\n",
    "    \"Bathroom\": \"bathroom\",\n",
    "    \"Bedroom\": \"bedroom\",\n",
    "    \"Kitchen\": \"kitchen\",\n",
    "    \"Livingroom\": \"living_room\",\n",
    "    \"Dinning\": \"other_real_estate\",\n",
    "    \"NaturalImages\": \"spam\",\n",
    "    \"SpamImages\": \"spam\"\n",
    "}\n",
    "\n",
    "# ĐƯỜNG DẪN ĐÚNG VỚI CẤU TRÚC CỦA BẠN\n",
    "BASE_DIR = \"data/train\"       # <--- SỬA TẠI ĐÂY\n",
    "\n",
    "for src, dst in mapping.items():\n",
    "    src_path = os.path.join(BASE_DIR, src)\n",
    "    dst_path = os.path.join(BASE_DIR, dst)\n",
    "\n",
    "    # Nếu thư mục nguồn không tồn tại, chỉ báo lỗi cảnh báo và bỏ qua\n",
    "    if not os.path.exists(src_path):\n",
    "        print(f\"⚠️  Skip, folder not found: {src_path}\")\n",
    "        continue\n",
    "\n",
    "    os.makedirs(dst_path, exist_ok=True)\n",
    "\n",
    "    for file in os.listdir(src_path):\n",
    "        shutil.move(os.path.join(src_path, file), os.path.join(dst_path, file))\n",
    "\n",
    "    shutil.rmtree(src_path)\n",
    "    print(f\"✔️ Moved {src} → {dst}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1377bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BƯỚC 2: Chia train / val / test ===\n",
      "Classes: ['bathroom', 'bedroom', 'kitchen', 'living_room', 'other_real_estate', 'spam']\n",
      "\n",
      "Class: bathroom - Total: 606, val: 90, test: 90\n",
      "\n",
      "Class: bedroom - Total: 1248, val: 187, test: 187\n",
      "\n",
      "Class: kitchen - Total: 965, val: 144, test: 144\n",
      "\n",
      "Class: living_room - Total: 1273, val: 190, test: 190\n",
      "\n",
      "Class: other_real_estate - Total: 1158, val: 173, test: 173\n",
      "\n",
      "Class: spam - Total: 1741, val: 261, test: 261\n",
      "\n",
      "Hoàn thành bước 2 (split train/val/test).\n",
      "\n",
      "✅ DONE. Cấu trúc data đã sẵn sàng để train.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# ---- CONFIG ----\n",
    "BASE_DATA_DIR = \"data\"          # thư mục gốc data\n",
    "SRC_TRAIN_DIR = os.path.join(BASE_DATA_DIR, \"train\")\n",
    "\n",
    "# tỉ lệ chia\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "def split_train_val_test():\n",
    "    print(\"=== BƯỚC 2: Chia train / val / test ===\")\n",
    "\n",
    "    # tạo folder val & test\n",
    "    val_dir = os.path.join(BASE_DATA_DIR, \"val\")\n",
    "    test_dir = os.path.join(BASE_DATA_DIR, \"test\")\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # các class sau khi rename\n",
    "    classes = [\n",
    "        d for d in os.listdir(SRC_TRAIN_DIR)\n",
    "        if os.path.isdir(os.path.join(SRC_TRAIN_DIR, d))\n",
    "    ]\n",
    "    print(\"Classes:\", classes)\n",
    "\n",
    "    for cls in classes:\n",
    "        cls_train_dir = os.path.join(SRC_TRAIN_DIR, cls)\n",
    "        cls_val_dir   = os.path.join(val_dir, cls)\n",
    "        cls_test_dir  = os.path.join(test_dir, cls)\n",
    "\n",
    "        os.makedirs(cls_val_dir, exist_ok=True)\n",
    "        os.makedirs(cls_test_dir, exist_ok=True)\n",
    "\n",
    "        files = [\n",
    "            f for f in os.listdir(cls_train_dir)\n",
    "            if os.path.isfile(os.path.join(cls_train_dir, f))\n",
    "        ]\n",
    "        random.shuffle(files)\n",
    "\n",
    "        n_total = len(files)\n",
    "        n_val = int(n_total * VAL_RATIO)\n",
    "        n_test = int(n_total * TEST_RATIO)\n",
    "\n",
    "        val_files  = files[:n_val]\n",
    "        test_files = files[n_val:n_val + n_test]\n",
    "        # phần còn lại giữ nguyên làm train\n",
    "        print(f\"\\nClass: {cls} - Total: {n_total}, val: {len(val_files)}, test: {len(test_files)}\")\n",
    "\n",
    "        # move val\n",
    "        for f in val_files:\n",
    "            src = os.path.join(cls_train_dir, f)\n",
    "            dst = os.path.join(cls_val_dir, f)\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "        # move test\n",
    "        for f in test_files:\n",
    "            src = os.path.join(cls_train_dir, f)\n",
    "            dst = os.path.join(cls_test_dir, f)\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "    print(\"\\nHoàn thành bước 2 (split train/val/test).\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(42)  # để kết quả chia cố định\n",
    "    split_train_val_test()\n",
    "    print(\"\\n DONE. Cấu trúc data đã sẵn sàng để train.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e38031b",
   "metadata": {},
   "source": [
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "368cfedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ======================\n",
    "# 1. CONFIG\n",
    "# ======================\n",
    "\n",
    "DATA_DIR = \"data\"  # thư mục chứa train/val/test\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 15\n",
    "LR = 1e-4\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "MODEL_OUT = \"realestate_resnet50_best.pth\"  # file sẽ save\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e49dfc",
   "metadata": {},
   "source": [
    "TRANSFORMS & DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3deefaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['bathroom', 'bedroom', 'kitchen', 'living_room', 'other_real_estate', 'spam']\n",
      "Num classes: 6\n"
     ]
    }
   ],
   "source": [
    "input_size = 224  # ResNet50 input\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "        std=[0.229, 0.224, 0.225]    # ImageNet std\n",
    "    ),\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dir = os.path.join(DATA_DIR, \"train\")\n",
    "val_dir   = os.path.join(DATA_DIR, \"val\")\n",
    "test_dir  = os.path.join(DATA_DIR, \"test\")\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "val_dataset   = datasets.ImageFolder(val_dir,   transform=val_test_transform)\n",
    "test_dataset  = datasets.ImageFolder(test_dir,  transform=val_test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"Num classes:\", num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7550d38c",
   "metadata": {},
   "source": [
    " MODEL (ResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc7252cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes: int):\n",
    "    # load ResNet50 đã pretrained trên ImageNet\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model(num_classes).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400eb0ee",
   "metadata": {},
   "source": [
    "TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f82d834b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Found bad image: data\\bad_images\\debut.jpg  -> move to data\\bad_images\\debut.jpg\n",
      "\n",
      " Done. Found & moved 1 bad images.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "BASE_DIR = Path(\"data\")  # sẽ quét cả train/val/test nếu có\n",
    "\n",
    "BAD_DIR = BASE_DIR / \"bad_images\"\n",
    "BAD_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def is_image_ok(path: Path) -> bool:\n",
    "    try:\n",
    "        with Image.open(path) as img:\n",
    "            img.verify()  # chỉ verify header, không load full\n",
    "        return True\n",
    "    except (UnidentifiedImageError, OSError):\n",
    "        return False\n",
    "\n",
    "def scan_and_move_bad_images():\n",
    "    count_bad = 0\n",
    "    for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.gif\"):\n",
    "        for img_path in BASE_DIR.rglob(ext):\n",
    "            if not img_path.is_file():\n",
    "                continue\n",
    "\n",
    "            if not is_image_ok(img_path):\n",
    "                count_bad += 1\n",
    "                # move ảnh lỗi sang thư mục bad_images để bạn còn xem lại\n",
    "                dest = BAD_DIR / img_path.name\n",
    "                print(f\"⚠️  Found bad image: {img_path}  -> move to {dest}\")\n",
    "                img_path.rename(dest)\n",
    "\n",
    "    print(f\"\\n Done. Found & moved {count_bad} bad images.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scan_and_move_bad_images()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a48a171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"data/train/spam/debut.jpg\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "300143b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/15\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/154 [00:06<17:19,  6.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 64\u001b[0m\n\u001b[0;32m     60\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(best_model_wts)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m---> 64\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 21\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     18\u001b[0m running_corrects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     19\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader):\n\u001b[0;32m     22\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     23\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "File \u001b[1;32mc:\\Users\\xamth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\xamth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    740\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\xamth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\xamth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\xamth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\xamth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32mc:\\Users\\xamth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xamth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:263\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpil_loader\u001b[39m(path: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 263\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\xamth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:3459\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pixels \u001b[38;5;241m>\u001b[39m MAX_IMAGE_PIXELS:\n\u001b[0;32m   3452\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   3453\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpixels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pixels) exceeds limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_IMAGE_PIXELS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pixels, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3454\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould be decompression bomb DOS attack.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3455\u001b[0m             DecompressionBombWarning,\n\u001b[0;32m   3456\u001b[0m         )\n\u001b[1;32m-> 3459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mopen\u001b[39m(\n\u001b[0;32m   3460\u001b[0m     fp: StrOrBytesPath \u001b[38;5;241m|\u001b[39m IO[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m   3461\u001b[0m     mode: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3462\u001b[0m     formats: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3463\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ImageFile\u001b[38;5;241m.\u001b[39mImageFile:\n\u001b[0;32m   3464\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3465\u001b[0m \u001b[38;5;124;03m    Opens and identifies the given image file.\u001b[39;00m\n\u001b[0;32m   3466\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3490\u001b[0m \u001b[38;5;124;03m    :exception TypeError: If ``formats`` is not ``None``, a list or a tuple.\u001b[39;00m\n\u001b[0;32m   3491\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=NUM_EPOCHS):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()\n",
    "                dataloader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            total = 0\n",
    "\n",
    "            for inputs, labels in tqdm(dataloader):\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                total += inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / total\n",
    "            epoch_acc = (running_corrects.double() / total).item()\n",
    "\n",
    "            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "            # lưu best model theo val_acc\n",
    "            if phase == \"val\" and epoch_acc > best_val_acc:\n",
    "                best_val_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                # SAVE MODEL Ở ĐÂY\n",
    "                torch.save({\n",
    "                    \"model_state_dict\": best_model_wts,\n",
    "                    \"class_names\": class_names,\n",
    "                }, MODEL_OUT)\n",
    "\n",
    "                print(f\" Saved best model (val_acc={best_val_acc:.4f}) to {MODEL_OUT}\")\n",
    "\n",
    "    print(f\"\\nBest val acc: {best_val_acc:.4f}\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = train_model(model, criterion, optimizer, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68e7a00",
   "metadata": {},
   "source": [
    "EVALUATE ON TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a32cbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Accuracy: 0.2794\n",
      "\n",
      " Done training. Best model saved at: realestate_resnet50_best.pth\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader, name=\"test\"):\n",
    "    model.eval()\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            total += inputs.size(0)\n",
    "\n",
    "    acc = (running_corrects.double() / total).item()\n",
    "    print(f\"{name} Accuracy: {acc:.4f}\")\n",
    "\n",
    "\n",
    "evaluate(model, test_loader, name=\"test\")\n",
    "\n",
    "print(\"\\n Done training. Best model saved at:\", MODEL_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992e61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: bathroom\n",
      "Prob : 0.9926713705062866\n",
      "Valid: True\n",
      "Reason: ok\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CHECKPOINT = \"realestate_resnet50_best.pth\"\n",
    "\n",
    "ckpt = torch.load(CHECKPOINT, map_location=DEVICE)\n",
    "CLASS_NAMES = ckpt[\"class_names\"]\n",
    "\n",
    "def create_model(num_classes: int):\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "model = create_model(num_classes=len(CLASS_NAMES))\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "input_size = 224\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "def laplacian_variance(cv2_img):\n",
    "    gray = cv2.cvtColor(cv2_img, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\n",
    "def predict_image(image_path: str):\n",
    "    # đọc ảnh bằng cv2 để check mờ/size\n",
    "    cv2_img = cv2.imread(image_path)\n",
    "    if cv2_img is None:\n",
    "        return \"spam\", 1.0, False, \"invalid_image\"\n",
    "\n",
    "    h, w = cv2_img.shape[:2]\n",
    "    if w < 600 or h < 400:\n",
    "        return \"spam\", 1.0, False, \"too_small\"\n",
    "\n",
    "    sharp = laplacian_variance(cv2_img)\n",
    "    if sharp < 50:\n",
    "        return \"spam\", 1.0, False, \"too_blurry\"\n",
    "\n",
    "    # chuyển sang PIL để transform\n",
    "    img_rgb = cv2.cvtColor(cv2_img, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(img_rgb)\n",
    "    input_tensor = transform(pil_img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        probs = torch.softmax(outputs, dim=1)[0]\n",
    "        prob_val, pred_idx = torch.max(probs, dim=0)\n",
    "\n",
    "    label = CLASS_NAMES[pred_idx.item()]\n",
    "    prob = prob_val.item()\n",
    "\n",
    "    # logic hợp lệ / spam\n",
    "    if label == \"spam\" and prob > 0.6:\n",
    "        return label, prob, False, \"spam\"\n",
    "    if prob < 0.4:\n",
    "        return \"spam\", prob, False, \"low_confidence\"\n",
    "\n",
    "    return label, prob, True, \"ok\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    img_path = \"data/train/mau-toilet-8.jpeg\"  \n",
    "    label, prob, is_valid, reason = predict_image(img_path)\n",
    "    print(\"Label:\", label)\n",
    "    print(\"Prob :\", prob)\n",
    "    print(\"Valid:\", is_valid)\n",
    "    print(\"Reason:\", reason)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
